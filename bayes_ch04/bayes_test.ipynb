{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.测试bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import bayes\n",
    "listOPosts,listClasses=bayes.loadDataSet()\n",
    "myVocabList=bayes.createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))\n",
    "p0V,p1V,pAb=bayes.trainNBO(trainMat,listClasses)\n",
    "bayes.spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "0.5\n",
      "[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.15948425 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -1.87180218]\n",
      "[-3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -1.94591015 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -1.94591015 -3.04452244 -1.65822808 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "['love', 'my', 'dalmation'] classified as : 0\n",
      "['stupid', 'garbage'] classified as : 1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print myVocabList\n",
    "print bayes.setOfWords2Vec(myVocabList,listOPosts[0])\n",
    "print pAb\n",
    "print p0V\n",
    "print p1V\n",
    "print bayes.testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['Hello', 'Since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'Google', 'Groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'February', '2011', 'We', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'Google', 'Groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'Instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'Google', 'Docs', 'and', 'Google', 'Sites', 'For', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'Google', 'Sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'You', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'If', 'you', 're', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'Google', 'Docs', 'You', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'Google', 'Groups', '']\n"
     ]
    }
   ],
   "source": [
    "mySent='This book is the best book on Python or M.L I have ever laid eyes upon'\n",
    "print mySent.split()\n",
    "import re\n",
    "regEx=re.compile('\\\\W*')\n",
    "listOfTokens=regEx.split(mySent)\n",
    "print listOfTokens\n",
    "data=[tok.lower() for tok in listOfTokens if len(tok)>0]\n",
    "print data\n",
    "emailText=open('email/ham/6.txt').read()\n",
    "print regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 初始化列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "n=9\n",
    "print [0]*n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数组去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my'])\n"
     ]
    }
   ],
   "source": [
    "dataset=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "         ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "         ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "         ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "         ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "         ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "#创建空集合\n",
    "vocabSet=set([])\n",
    "for document in dataset:\n",
    "    vocabSet=vocabSet | set(document)\n",
    "print vocabSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 列表 for语句过滤列表值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "listOfTokens=['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
    "data=[tok.lower() for tok in listOfTokens if len(tok)>0]\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 留存交叉验证的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[19]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18]\n",
      "[19, 17]\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18]\n",
      "[19, 17, 6]\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18]\n",
      "[19, 17, 6, 14]\n",
      "[0, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 15, 16, 18]\n",
      "[19, 17, 6, 14, 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "trainSet=range(20)\n",
    "testSet=[]\n",
    "for i in range(5):\n",
    "    randIndex=int(np.random.uniform(0,len(trainSet)))\n",
    "    testSet.append(trainSet[randIndex])\n",
    "    del(trainSet[randIndex])\n",
    "    print trainSet\n",
    "    print testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
